{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpliceJunctionClassified V0.1 (Updated Jan 2024)\n",
    "# Written by Yang Li Nov-2023\n",
    "\n",
    "def tx_by_gene(gtf_annot):\n",
    "    transcripts_by_gene = {}\n",
    "    i = 1\n",
    "    for dic in parse_gtf(gtf_annot):\n",
    "        if 'appris_principal' not in dic['info']:\n",
    "            continue\n",
    "        if dic['type'] == 'transcript':\n",
    "            if dic['gene_name'] not in transcripts_by_gene:\n",
    "                transcripts_by_gene[dic['gene_name']] = {dic['transcript_name']: []}\n",
    "            else:\n",
    "                transcripts_by_gene[dic['gene_name']] = transcripts_by_gene[dic['gene_name']] | {dic['transcript_name']: []}\n",
    "        if dic['type'] == 'start_codon':\n",
    "            insort(transcripts_by_gene[dic['gene_name']][dic['transcript_name']], (dic['end']))\n",
    "        if dic['type'] == 'stop_codon':\n",
    "            insort(transcripts_by_gene[dic['gene_name']][dic['transcript_name']], (dic['start']))\n",
    "        if dic['type'] == 'CDS':\n",
    "            insort(transcripts_by_gene[dic['gene_name']][dic['transcript_name']], (dic['start']))\n",
    "            insort(transcripts_by_gene[dic['gene_name']][dic['transcript_name']], (dic['end']))\n",
    "    return transcripts_by_gene\n",
    "\n",
    "def ptc_pos_from_prot(prot, sub):\n",
    "    to_return = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = prot.find(sub, start)\n",
    "        if start == -1: return to_return\n",
    "        else:\n",
    "            to_return.append(start)\n",
    "        start += 1\n",
    "\n",
    "def many_junctions(failing_juncs, gene_name, transcripts_by_gene, strand, chrom):\n",
    "\n",
    "    many_exons_before = []\n",
    "    many_exons_after = []\n",
    "    for junc in failing_juncs:\n",
    "        junc_added = False\n",
    "        possible_transcripts = transcripts_by_gene[gene_name]\n",
    "        for transcript in possible_transcripts:\n",
    "            s = list(possible_transcripts[transcript])\n",
    "            #cast to list of integers\n",
    "            s = [eval(j) for j in s]\n",
    "            s.pop(len(s)-2)\n",
    "            s.pop(1)\n",
    "            if strand == '-':\n",
    "                s.reverse()\n",
    "\n",
    "            new_junc = False\n",
    "            failing_junc = False\n",
    "            for i in range(len(s)-2, 0, -2):\n",
    "                junction = tuple([s[i-1], s[i]])\n",
    "\n",
    "                #overlapping 3' or 5' splice site, just replace the junction\n",
    "                if junc[0] == junction[0] or junc[1] == junction[1]:\n",
    "                    s[i-1] = junc[0]\n",
    "                    s[i] = junc[1]\n",
    "                    new_junc = True\n",
    "                    print(i)\n",
    "                #if this does not fit in our transcript, move on\n",
    "                    if strand == '+' and not (s[i-1] > s[i-2] and s[i] < s[i+1]):\n",
    "                        failing_junc = True\n",
    "                    elif strand == '-' and not (s[i-1] < s[i-2] and s[i] > s[i+1]):\n",
    "                        failing_junc = True\n",
    "                    \n",
    "                    break  \n",
    "            #If our overlapping site configuration fails in the transcript, skip this transcript     \n",
    "            if failing_junc:\n",
    "                continue\n",
    "            s.reverse()\n",
    "            allprot = Seq(\"\")\n",
    "            leftover = Seq(\"\")\n",
    "\n",
    "            bool_exons_after = False\n",
    "            bool_exons_before = False\n",
    "            if not new_junc:\n",
    "                continue\n",
    "            for i in range(0, len(s)-1, 2):\n",
    "                exon_coord = s[i:i+2]\n",
    "                exon_coord.sort()\n",
    "                exon_coord = tuple(exon_coord)\n",
    "                exlen = int(exon_coord[1])-int(exon_coord[0])\n",
    "\n",
    "\n",
    "                \"\"\"Quinn Comment: find start position relative to named start of this exon and translate to protein\"\"\"\n",
    "                \"\"\"Quinn Comment: Coordinates from PERIND file and GTF file are exon start and end coordinates, so \n",
    "                we must add 1 to length\"\"\"\n",
    "                startpos = (len(leftover)+exlen+1)%3\n",
    "\n",
    "                if strand == '+':\n",
    "                    seq = Seq(fa.fetch(chrom, (exon_coord[0],exon_coord[1])))+leftover \n",
    "                    prot = seq[startpos:].translate()\n",
    "                    leftover = seq[:startpos]                                                                                                               \n",
    "\n",
    "                    if i == 0:\n",
    "                        bool_ptc = \"*\" in prot[:-1]\n",
    "                        ptc_pos = ptc_pos_from_prot(prot[:-1], '*')\n",
    "                    else:\n",
    "                        bool_ptc = \"*\" in prot \n",
    "                        ptc_pos = ptc_pos_from_prot(prot, '*')\n",
    "\n",
    "                    allprot = prot+allprot\n",
    "                else:\n",
    "                    seq = leftover+Seq(fa.fetch(chrom, (exon_coord[0],exon_coord[1])))\n",
    "                    aseq = seq\n",
    "                    if startpos > 0:\n",
    "                        leftover = seq[-startpos:]\n",
    "                    else:\n",
    "                        leftover = Seq(\"\")\n",
    "                    seq = seq.reverse_complement()\n",
    "                    prot = seq[startpos:].translate()\n",
    "                    if i == 0:\n",
    "                        bool_ptc = \"*\" in prot[:-1]\n",
    "                        #ptc_pos = ptc_pos_from_prot(prot[:-1], '*')\n",
    "                    else:\n",
    "                        bool_ptc = \"*\" in prot\n",
    "                        #ptc_pos = ptc_pos_from_prot(prot, '*')\n",
    "\n",
    "                    allprot = prot+allprot\n",
    "                \n",
    "                #store a long_exon tag, only add this junction to list if it goes on to cause no PTCs that are not in long exons\n",
    "\n",
    "                exons_after = i/2\n",
    "                exons_before = (len(s) - 2)/2 - i/2\n",
    "                if bool_ptc and i != 0:\n",
    "                    #ptc_coords = [exon_coord[0] + startpos + 3*x for x in ptc_pos]\n",
    "                    if exons_before > 4:\n",
    "                        bool_exons_before = True\n",
    "                    if exons_after > 4:\n",
    "                        bool_exons_after = True\n",
    "                    else:\n",
    "                        break\n",
    "                if bool_exons_before or bool_exons_after and i == len(s) - 2:\n",
    "                    if bool_exons_before:\n",
    "                        many_exons_before.append(junc)\n",
    "                    if bool_exons_after:\n",
    "                        many_exons_after.append(junc)\n",
    "                    junc_added = True\n",
    "            if junc_added:\n",
    "                break\n",
    "    return many_exons_before, many_exons_after\n",
    "\n",
    "\n",
    "def long_exon_finder(failing_juncs, gene_name, transcripts_by_gene, strand, chrom):\n",
    "    long_exons = []\n",
    "    for junc in failing_juncs:\n",
    "        junc_added = False\n",
    "        possible_transcripts = transcripts_by_gene[gene_name]\n",
    "        for transcript in possible_transcripts:\n",
    "            s = list(possible_transcripts[transcript])\n",
    "            #cast to list of integers\n",
    "            s = [eval(j) for j in s]\n",
    "            \n",
    "            if strand == '-':\n",
    "                s.reverse()\n",
    "\n",
    "            new_junc = False\n",
    "            failing_junc = False\n",
    "            for i in range(len(s)-2, 0, -2):\n",
    "                junction = tuple([s[i-1], s[i]])\n",
    "\n",
    "                #overlapping 3' or 5' splice site, just replace the junction\n",
    "                if junc[0] == junction[0] or junc[1] == junction[1]:\n",
    "                    s[i-1] = junc[0]\n",
    "                    s[i] = junc[1]\n",
    "                    new_junc = True\n",
    "                #if this does not fit in our transcript, move on\n",
    "                    if strand == '+' and not (s[i-1] > s[i-2] and s[i] < s[i+1]):\n",
    "                        failing_junc = True\n",
    "                    elif strand == '-' and not (s[i-1] < s[i-2] and s[i] > s[i+1]):\n",
    "                        failing_junc = True\n",
    "                    \n",
    "                    break  \n",
    "\n",
    "            #If our overlapping site configuration fails in the transcript, skip this transcript     \n",
    "            if failing_junc:\n",
    "                continue\n",
    "            \n",
    "            s.reverse()\n",
    "            allprot = Seq(\"\")\n",
    "            leftover = Seq(\"\")\n",
    "\n",
    "            bool_long_exon = False\n",
    "            for i in range(0, len(s)-1, 2):\n",
    "                exon_coord = s[i:i+2]\n",
    "                exon_coord.sort()\n",
    "                exon_coord = tuple(exon_coord)\n",
    "                exlen = int(exon_coord[1])-int(exon_coord[0])\n",
    "\n",
    "\n",
    "                \"\"\"Quinn Comment: find start position relative to named start of this exon and translate to protein\"\"\"\n",
    "                \"\"\"Quinn Comment: Coordinates from PERIND file and GTF file are exon start and end coordinates, so \n",
    "                we must add 1 to length\"\"\"\n",
    "                startpos = (len(leftover)+exlen+1)%3\n",
    "\n",
    "                if strand == '+':\n",
    "                    seq = Seq(fa.fetch(chrom, (exon_coord[0],exon_coord[1])))+leftover \n",
    "                    prot = seq[startpos:].translate()\n",
    "                    leftover = seq[:startpos]                                                                                                               \n",
    "\n",
    "                    if i == 0:\n",
    "                        bool_ptc = \"*\" in prot[:-1]\n",
    "                        ptc_pos = ptc_pos_from_prot(prot[:-1], '*')\n",
    "                    else:\n",
    "                        bool_ptc = \"*\" in prot \n",
    "                        ptc_pos = ptc_pos_from_prot(prot, '*')\n",
    "\n",
    "                    allprot = prot+allprot\n",
    "                else:\n",
    "                    seq = leftover+Seq(fa.fetch(chrom, (exon_coord[0],exon_coord[1])))\n",
    "                    aseq = seq\n",
    "                    if startpos > 0:\n",
    "                        leftover = seq[-startpos:]\n",
    "                    else:\n",
    "                        leftover = Seq(\"\")\n",
    "                    seq = seq.reverse_complement()\n",
    "                    prot = seq[startpos:].translate()\n",
    "                    if i == 0:\n",
    "                        bool_ptc = \"*\" in prot[:-1]\n",
    "                        ptc_pos = ptc_pos_from_prot(prot[:-1], '*')\n",
    "                    else:\n",
    "                        bool_ptc = \"*\" in prot\n",
    "                        ptc_pos = ptc_pos_from_prot(prot, '*')\n",
    "\n",
    "                    allprot = prot+allprot\n",
    "                \n",
    "                #store a long_exon tag, only add this junction to list if it goes on to cause no PTCs that are not in long exons\n",
    "                if bool_ptc and new_junc and i != 0:\n",
    "                    ptc_coords = [exon_coord[0] + startpos + 3*x for x in ptc_pos]\n",
    "                    if exlen+1 > 407:\n",
    "                        bool_long_exon = True\n",
    "                    else:\n",
    "                        break\n",
    "                if bool_long_exon and i == len(s) - 2:\n",
    "                    long_exons.append(junc)\n",
    "                    junc_added = True\n",
    "            if junc_added:\n",
    "                break\n",
    "    return long_exons\n",
    "\n",
    "def check_utrs(junc,utrs):\n",
    "    '''\n",
    "    checks if junction is close or within 100bp of UTRs\n",
    "    '''\n",
    "    for s1,s2 in list(utrs):\n",
    "        if abs(junc[0]-s1) < 100 or abs(junc[1]-s2) < 100:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def solve_NMD(chrom, strand, junc, start_codons, stop_codons,gene_name, \n",
    "              verbose = False, exonLcutoff = 1000):\n",
    "    '''\n",
    "    Compute whether there is a possible combination that uses the junction without\n",
    "    inducing a PTC. We start with all annotated stop codon and go backwards.\n",
    "    '''\n",
    "\n",
    "    global fa\n",
    "    \n",
    "    seed = []\n",
    "\n",
    "    junc.sort()\n",
    "    if strand == \"+\":\n",
    "        junc.reverse()\n",
    "        \n",
    "    \"\"\"Quinn Comment: Adds all 'stop codons' to a nested list called seed\"\"\" \n",
    "    ##in an individual transcript    \n",
    "    for c in stop_codons:\n",
    "        if strand == \"+\":\n",
    "            seed.append([c[1]])\n",
    "        else:\n",
    "            seed.append([c[0]])\n",
    "\n",
    "    # seed starts with just stop codon and then a possible 3'ss-5'ss junction\n",
    "    # without introducing a PTC [stop_codon,3'ss, 5'ss, 3'ss, ..., start_codon]\n",
    "    \n",
    "    seq_db = {}\n",
    "    junc_pass = {}\n",
    "    junc_fail = {}\n",
    "    path_pass = []\n",
    "    proteins = []\n",
    "    \n",
    "    dic_terminus = {}\n",
    "    dic_paths = {}\n",
    "\n",
    "    depth = 0\n",
    "\n",
    "    \"\"\"Quinn Comment: while our seed length is greater than 0 - which means we have charted all possible paths through \n",
    "    all junctions ending in a stop codon (or there is an exon longer than 1000 bp and we have no complete paths)\"\"\"\n",
    "    while len(seed) > 0:\n",
    "        new_seed = []\n",
    "        final_check = []\n",
    "        depth += 1\n",
    "        if verbose:\n",
    "            sys.stdout.write(\"Depth %s, Seed L = %s\\n\"%(depth, len(seed)))\n",
    "        #print(start_codons, [s[-1] for s in seed][-10:], len(junc))\n",
    "        framepos = {}\n",
    "                    \n",
    "        for s in seed:\n",
    "            # first check that the seed paths are good        \n",
    "            bool_ptc = False\n",
    "            leftover = ''\n",
    "            if len(s) > 0:                \n",
    "                leftover = Seq(\"\")\n",
    "                allprot = Seq(\"\")\n",
    "\n",
    "                \"\"\"Quinn Comment: loop through the exons, calculating lengths\"\"\"\n",
    "                for i in range(0, len(s)-1, 2):\n",
    "                    exon_coord = s[i:i+2]\n",
    "                    exon_coord.sort()\n",
    "                    exon_coord = tuple(exon_coord)\n",
    "                    exlen = exon_coord[1]-exon_coord[0]\n",
    "\n",
    "\n",
    "                    \"\"\"Quinn Comment: find start position relative to named start of this exon and translate to protein\"\"\"\n",
    "                    \"\"\"Quinn Comment: Coordinates from PERIND file and GTF file are exon start and end coordinates, so \n",
    "                    we must add 1 to length\"\"\"\n",
    "                    startpos = (len(leftover)+exlen+1)%3\n",
    "                    if strand == '+':\n",
    "                        seq = Seq(fa.fetch(chrom, (exon_coord[0],exon_coord[1])))+leftover \n",
    "                        prot = seq[startpos:].translate()\n",
    "                        leftover = seq[:startpos]                                                                                                               \n",
    "                        allprot = prot+allprot  \n",
    "                    else:\n",
    "                        seq = leftover+Seq(fa.fetch(chrom, (exon_coord[0],exon_coord[1])))\n",
    "                        aseq = seq\n",
    "                        if startpos > 0:\n",
    "                            leftover = seq[-startpos:]\n",
    "                        else:\n",
    "                            leftover = Seq(\"\")\n",
    "                        seq = seq.reverse_complement()\n",
    "                        prot = seq[startpos:].translate()\n",
    "                        allprot = prot+allprot\n",
    "\n",
    "                    #found a PTC in this transcript if any element but the last is a stop codon    \n",
    "                    bool_ptc = \"*\" in allprot[:-1]\n",
    "\n",
    "            \"\"\"Quinn Comment: if we found a PTC, add all intron coordinate pairs involved in the transcript to junc_fail\"\"\"        \n",
    "            if bool_ptc:\n",
    "                #This transcript failed\n",
    "                for i in range(1, len(s)-1, 2):                                                                                                                  \n",
    "                    j_coord = s[i:i+2]                                                                                                                           \n",
    "                    j_coord.sort()                                                                                                                             \n",
    "                    j_coord = tuple(j_coord)                                                                                                                     \n",
    "                    if j_coord not in junc_fail:                                                                                                                 \n",
    "                        junc_fail[j_coord] = 0                                                                                                                   \n",
    "                    junc_fail[j_coord] += 1  \n",
    "\n",
    "                continue\n",
    "        \n",
    "            # passed\n",
    "            \"\"\"Quinn Comment: if we don't just have a stop codon, create a terminus for this \n",
    "            seed at the last 3' splice site or start codon; terminus is last two coordinates and the reading frame, \n",
    "            used for dynamic programming later\"\"\"\n",
    "            if len(s) > 2:\n",
    "                terminus = (s[-2],s[-1],leftover)\n",
    "                \n",
    "                if terminus in dic_terminus:\n",
    "                    dic_terminus[terminus].append(tuple(s))\n",
    "                    continue\n",
    "                else:\n",
    "                    dic_terminus[terminus] = [tuple(s)]\n",
    "            \n",
    "            last_pos = s[-1]\n",
    "            \n",
    "            \"\"\"Quinn Comment: check the last position of our seed to see if it is close to a start codon, within a potential exon's length,\n",
    "             and add your seed plus this start codon to final_check \"\"\"\n",
    "            for start in start_codons:                \n",
    "                #print(\"start\", start, abs(last_pos-start[0]))\n",
    "                if strand == \"+\" and last_pos > start[0] and abs(last_pos-start[0]) < exonLcutoff:\n",
    "                    final_check.append(s+[start[0]])\n",
    "                elif strand == \"-\" and last_pos < start[1] and abs(last_pos-start[1]) < exonLcutoff:\n",
    "                    final_check.append(s+[start[1]]) \n",
    "\n",
    "            \"\"\"Quinn Comment: add all possible places to go from our last_pos to the seed (nested list)\"\"\"\n",
    "            for j0,j1 in junc:                \n",
    "                if strand == \"+\" and last_pos > j1 and abs(last_pos-j1) < exonLcutoff:\n",
    "                    new_seed.append(s+[j1,j0])\n",
    "                #print(\"junction\", (j0,j1), abs(last_pos-j0))\n",
    "                if strand == \"-\" and last_pos < j0 and abs(last_pos-j0) < exonLcutoff: \n",
    "                    new_seed.append(s+[j0,j1])\n",
    "                    \n",
    "        \"\"\"Quinn Comment: Exited from s in seed loop, now we check our final_checks of the full paths, we do not\n",
    "        eliminate paths based on presence of a PTC, rather we classify full complete paths without PTCs if they exist\"\"\"\n",
    "        # check that the possible final paths are good\n",
    "        for s in final_check:\n",
    "            leftover = Seq(\"\")\n",
    "            allprot = Seq(\"\")\n",
    "            for i in range(0, len(s)-1, 2):\n",
    "                exon_coord = s[i:i+2]\n",
    "                exon_coord.sort()\n",
    "                exon_coord = tuple(exon_coord)\n",
    "                exlen = exon_coord[1]-exon_coord[0]\n",
    "                startpos = (len(leftover)+exlen+1)%3\n",
    "                if strand == \"+\":\n",
    "                    seq = Seq(fa.fetch(chrom, (exon_coord[0],exon_coord[1])))+leftover\n",
    "                    leftover = seq[:startpos]  \n",
    "                    prot = seq[startpos:].translate()\n",
    "                    allprot = prot+allprot\n",
    "                else:\n",
    "                    seq = leftover+Seq(fa.fetch(chrom, (exon_coord[0],exon_coord[1])))\n",
    "                    if startpos > 0:                                                                                                    \n",
    "                        leftover = seq[-startpos:]                                    \n",
    "                    else:\n",
    "                        leftover = Seq(\"\")\n",
    "                    seq = seq.reverse_complement()                                                                                                           \n",
    "                    prot = seq[startpos:].translate()                                                                                                        \n",
    "                    allprot = prot+allprot                    \n",
    "            bool_ptc = \"*\" in allprot[:-1]\n",
    "        \n",
    "            \"\"\"Quinn Comment: Classify seed + start codon as a passing path if no PTCs found in previous block of code\"\"\"\n",
    "            if not bool_ptc:\n",
    "                # all pass\n",
    "                proteins.append(\"\\t\".join([gene_name,chrom,strand, \"-\".join([str(x) for x in s]), str(allprot)])+'\\n')\n",
    "                #print(\"ALL PASS %s\"%(s))\n",
    "                path_pass.append(tuple(s))\n",
    "                for i in range(1, len(s), 2):\n",
    "                    j_coord = s[i:i+2]\n",
    "                    j_coord.sort()\n",
    "                    j_coord = tuple(j_coord)\n",
    "                    if j_coord not in junc_pass:\n",
    "                        junc_pass[j_coord] = 0\n",
    "                    junc_pass[j_coord] += 1\n",
    "\n",
    "        seed = new_seed\n",
    "    \n",
    "    \n",
    "    \"\"\"Quinn Comment: OUT OF WHILE LOOP through all possible paths/seeds; \n",
    "    check all terminus' to see if they are part of a full path that has been classified as passing\"\"\"\n",
    "    while True:\n",
    "        new_paths = []\n",
    "        for terminus in dic_terminus:\n",
    "            terminus_pass = False\n",
    "            for path_subset in dic_terminus[terminus]:\n",
    "                for path in path_pass:\n",
    "                    if path[:len(path_subset)] == path_subset:\n",
    "                        terminus_pass = True\n",
    "                        break\n",
    "            #print(terminus, terminus_pass)\n",
    "\n",
    "            \"\"\"Quinn Comment: if our terminus is part of a passing path, we want to make sure if is reflected in passing paths and\n",
    "            add the associate junctions to junc_pass, only if they are not present\"\"\"\n",
    "            if terminus_pass:\n",
    "                for path_subset in dic_terminus[terminus]:\n",
    "                    if path_subset in path_pass: continue\n",
    "                    new_paths.append(path_subset)\n",
    "                    path_pass.append(path_subset)\n",
    "                    for i in range(1, len(path_subset), 2):\n",
    "                        j_coord = list(path_subset[i:i+2])\n",
    "                        j_coord.sort()\n",
    "                        j_coord = tuple(j_coord)\n",
    "                        if j_coord not in junc_pass:\n",
    "                            junc_pass[j_coord] = 0\n",
    "                            if verbose:\n",
    "                                sys.stdout.write(\"junction %s pass\\n\"%j_coord)\n",
    "        \"\"\"Quinn Comment: we could have a new path_pass added, so our while loop checks again to see if there are any new paths \n",
    "        that are now going to be passing considering our additions\"\"\"\n",
    "        if len(new_paths) == 0:\n",
    "            break\n",
    "            \n",
    "    return junc_pass,junc_fail,proteins\n",
    "\n",
    "def parse_gtf(gtf: str):\n",
    "    '''Lower level function to parse GTF file\n",
    "    - gtf: str : path to GTF annotation file\n",
    "    - returns: dictionary with keys: \n",
    "        chrom, source, type, start, end, strand, frame, info, gene_name, transcript_type, transcript_name, gene_type\n",
    "    '''\n",
    "    fields = [\"chrom\", \"source\", \"type\", \"start\", \"end\", \".\",\"strand\",\"frame\", \"info\"]\n",
    "    open_gtf = lambda x: gzip.open(x) if \".gz\" in x else open(x)\n",
    "    for ln in open_gtf(gtf):\n",
    "        ln = ln.decode('ascii') if \".gz\" in gtf else ln\n",
    "        dic = {}\n",
    "        if ln[0] == \"#\": continue\n",
    "        ln = ln.strip().split('\\t')\n",
    "        for i in range(len(fields)):\n",
    "            dic[fields[i]] = ln[i]\n",
    "\n",
    "        # add 4 additional fields, parsed from info field\n",
    "        for ks in ['gene_name', \"transcript_type\",\"transcript_name\", \"gene_type\"]:\n",
    "            info_fields = [{x.split()[0]: x.split()[1].replace('\"', '')} \n",
    "                          for x in dic['info'].split(';') if len(x.split()) > 1]\n",
    "            info_fields = {k: v for d in info_fields for k, v in d.items()}\n",
    "            try: \n",
    "                dic[ks] = info_fields[ks]\n",
    "            except:\n",
    "                dic[ks] = None # if line is a gene, then wont have transcript info\n",
    "        yield dic\n",
    "         \n",
    "\n",
    "def parse_annotation(gtf_annot: str):\n",
    "    '''\n",
    "    Used `parse_gtf` to first parse a gtf file, then extract and return further\n",
    "    information, including gene coordinates, intron info, and splice site to \n",
    "    gene-name dictionary.\n",
    "    \n",
    "    - gtf_anno: str : path to GTF annotation file\n",
    "    - returns: \n",
    "        - genes_coords: gene coordinates, grouped by chromosome and strand \n",
    "        - introns_info: a dictionary with these keys: junctions (ie all introns),\n",
    "                        start_codon, stop_codon, utrs, pcjunctions (ie only protein coding introns) \n",
    "        - ss2gene: a dictionary with splice site as keys, and gene name as values, eg. ('chr1', 11869): 'DDX11L1'\n",
    "    '''\n",
    "    genes_info = {}\n",
    "    introns_info = {}\n",
    "    genes_coords = {}\n",
    "    ss2gene = {}\n",
    "\n",
    "    for dic in parse_gtf(gtf_annot):\n",
    "        chrom = dic['chrom']\n",
    "        gname = dic['gene_name']\n",
    "        tname = dic['transcript_name'], gname\n",
    "        anntype  = dic['type']\n",
    "        if dic['type'] == 'gene': \n",
    "            dic['transcript_type'] = \"gene\"\n",
    "        if dic['transcript_type'] == \"nonsense_mediated_decay\" and anntype == \"stop_codon\": \n",
    "            continue \n",
    "        if dic['transcript_type'] != \"protein_coding\" and anntype == \"UTR\": \n",
    "            continue\n",
    "        \"\"\"Quinn: CONVERT TO BED FORMAT IS ERROR\"\"\"\n",
    "        start, end = int(dic['start']), int(dic['end']) # convert to BED format\n",
    "        strand = dic['strand']\n",
    "    \n",
    "        if (chrom, strand) not in genes_coords:\n",
    "            genes_coords[(chrom, strand)] = []\n",
    "\n",
    "        if tname not in genes_info: # tname is (transcript_name, gene_name)\n",
    "            genes_info[tname] = {'exons':[],\n",
    "                                 'start_codon':set(), \n",
    "                                 'stop_codon':set(), \n",
    "                                 'utrs':set(),\n",
    "                                 'type':dic['transcript_type']\n",
    "                                 }\n",
    "        \n",
    "        if anntype in [\"start_codon\", \"stop_codon\"]:\n",
    "            genes_info[tname][anntype].add((start,end))\n",
    "        elif anntype in [\"gene\"]:\n",
    "            genes_coords[(chrom,strand)].append(((start,end), gname))\n",
    "        elif anntype in ['exon']:\n",
    "            genes_info[tname]['exons'].append((start,end))\n",
    "            # Store gene info for splice sites\n",
    "            ss2gene[(chrom, int(dic['start']) - 1)] = dic['gene_name'] # BED\n",
    "            ss2gene[(chrom, int(dic['end']))] = dic['gene_name'] # BED\n",
    "\n",
    "        elif anntype in ['UTR']:\n",
    "            genes_info[tname]['utrs'].add((start,end))\n",
    "\n",
    "    for gene in genes_info: # this is actually transcript level!!! (transcript_name, gene_name)\n",
    "        gene_name = gene[1]\n",
    "        exons = genes_info[gene]['exons']\n",
    "        exons.sort()\n",
    "\n",
    "        pc = False # Basically use transcript_type == protein_coding to set flag\n",
    "        if genes_info[gene]['type'] == \"protein_coding\": # AGAIN here gene = (transcript_name, gene_name)\n",
    "            pc = True\n",
    "\n",
    "        junctions = set() # all junctions/introns\n",
    "        pcjunctions = set() # only protein coding junctions/introns\n",
    "\n",
    "        for i in range(len(exons)-1):\n",
    "            intron = (exons[i][1], exons[i+1][0])\n",
    "            junctions.add(intron)\n",
    "            if pc:\n",
    "                pcjunctions.add(intron)\n",
    "\n",
    "        if gene_name not in introns_info: # here only gene_name, does not incl. transcript_name\n",
    "            introns_info[gene_name] = {'junctions':set(),\n",
    "                                       'start_codon':set(),\n",
    "                                       'stop_codon':set(),\n",
    "                                       'utrs':set(),\n",
    "                                       'pcjunctions':set(),\n",
    "                                 }\n",
    "\n",
    "        introns_info[gene_name]['start_codon'] = introns_info[gene_name]['start_codon'].union(genes_info[gene]['start_codon'])\n",
    "        introns_info[gene_name]['stop_codon'] = introns_info[gene_name]['stop_codon'].union(genes_info[gene]['stop_codon'])\n",
    "        introns_info[gene_name]['junctions'] = introns_info[gene_name]['junctions'].union(junctions)\n",
    "        introns_info[gene_name]['utrs'] = introns_info[gene_name]['utrs'].union(genes_info[gene]['utrs'])\n",
    "        introns_info[gene_name]['pcjunctions'] = introns_info[gene_name]['pcjunctions'].union(pcjunctions)\n",
    "\n",
    "    return genes_coords, introns_info, ss2gene\n",
    "\n",
    "\n",
    "\n",
    "# def getmean(lst):\n",
    "#     return sum(lst)/float(len(lst))\n",
    "\n",
    "# def getmedian(lst):\n",
    "#     lst.sort()\n",
    "#     n = len(lst)\n",
    "#     if n < 1:\n",
    "#         return None\n",
    "#     if n % 2 == 1:\n",
    "#         return lst[n//2]\n",
    "#     else:\n",
    "#         return sum(lst[n//2-1:n//2+1])/2.0\n",
    "\n",
    "def get_feature(fname, feature = \"exon\"):\n",
    "    ss2gene = {}\n",
    "    if \".gz\" in fname:\n",
    "        F = gzip.open(fname)\n",
    "    else:\n",
    "        F = open(fname)\n",
    "    for ln in F:\n",
    "        if ln[0] == \"#\": continue\n",
    "        ln = ln.split('\\t')\n",
    "        gID = ln[-1].split('gene_name \"')[1].split('\"')[0]\n",
    "\n",
    "        if ln[2] != feature:\n",
    "            continue\n",
    "        ss2gene[(ln[0], int(ln[3]))] = gID\n",
    "        ss2gene[(ln[0], int(ln[4]))] = gID\n",
    "    return ss2gene\n",
    "\n",
    "\n",
    "def get_overlap_stream(L1, L2, relax = 0):\n",
    "    '''                                                                                                                                                                \n",
    "    L1 and L2 are sorted lists of tuples with key, values                                                                                                              \n",
    "    '''\n",
    "    i, j = 0, 0\n",
    "    while i < len(L1) and j < len(L2):\n",
    "        if L1[i][0][1] < L2[j][0][0]:\n",
    "            i += 1\n",
    "            continue\n",
    "        elif L2[j][0][1] < L1[i][0][0]:\n",
    "            j += 1\n",
    "            continue\n",
    "        else:\n",
    "            k = 0\n",
    "            # hits overlapping, check all L2 that may overlap with intron                                                                                              \n",
    "            while L2[j+k][0][0] <= L1[i][0][1]:\n",
    "                if overlaps(L1[i][0], L2[j+k][0]):\n",
    "                    yield L1[i], L2[j+k]\n",
    "                k += 1\n",
    "                if j+k == len(L2): break\n",
    "            i += 1\n",
    "\n",
    "def overlaps(A: tuple, B: tuple):\n",
    "    '''                                                                                                                                                                \n",
    "    Checks if A and B overlaps                                                                                                                                         \n",
    "    A: tuple : (start, end)\n",
    "    B: tuple : (start, end)\n",
    "    '''\n",
    "    if A[1] < B[0] or B[1] < A[0]:\n",
    "        return False\n",
    "    else: return True\n",
    "\n",
    "\n",
    "def ClassifySpliceJunction(options):\n",
    "    '''\n",
    "        - perind_file: str : path to counts file, e.g. leafcutter_perind.counts.gz\n",
    "        - gtf_annot: str : Annotation GTF file, for example gencode.v37.annotation.gtf.gz\n",
    "        - rundir: str : run directory, default is current directory\n",
    "    '''\n",
    "\n",
    "    gtf_annot, rundir, outprefix = options.annot, options.rundir, options.outprefix\n",
    "    verbose = False or options.verbose\n",
    "    if options.countfile is None:\n",
    "        perind_file = f\"{rundir}/{outprefix}_perind.counts.gz\"\n",
    "    else:\n",
    "        perind_file = options.countfile\n",
    "\n",
    "    # read leafcutter perind file and store junctions in dictionary: dic_junc\n",
    "    # key = (chrom,strand), value = list of junctions [(start,end)]\n",
    "    dic_junc = {}\n",
    "    sys.stdout.write(f\"Processing junction counts {perind_file}...\")\n",
    "    for ln in gzip.open(perind_file):\n",
    "        junc_info = ln.decode('ascii').split()[0] # first column\n",
    "        if junc_info == \"chrom\": continue # header\n",
    "        \n",
    "        chrom, start, end, clu_strand = junc_info.split(\":\")\n",
    "        strand = clu_strand.split(\"_\")[-1]\n",
    "        if (chrom,strand) not in dic_junc: \n",
    "            dic_junc[(chrom,strand)] = []\n",
    "        dic_junc[(chrom,strand)].append((int(start), int(end)))\n",
    "\n",
    "    sys.stdout.write(\"done!\\n\")\n",
    "    if verbose:\n",
    "        sys.stdout.write(\"Processed: \")\n",
    "        for chrstrand in dic_junc:\n",
    "            sys.stdout.write(f\"{len(dic_junc[chrstrand])} jxns on {chrstrand[0]} ({chrstrand[1]}).\")\n",
    "\n",
    "    \n",
    "    # load or parse gtf annotations\n",
    "    # g_coords: gene coordinates, grouped by chromosome and strand\n",
    "    # g_info: a dictionary with (transcript_name, gene_name) as keys, and intron info as values\n",
    "    try: \n",
    "        sys.stdout.write(\"Loading annotations...\\n\")\n",
    "        parsed_gtf = f\"{rundir}/{gtf_annot.split('/')[-1].split('.gtf')[0]}_SJC_annotations.pckle\"\n",
    "        with open(parsed_gtf, 'rb') as f:\n",
    "            g_coords, g_info = pickle.load(f)\n",
    "        sys.stdout.write(\"done!\\n\")\n",
    "    except:\n",
    "        sys.stdout.write(\"Parsing annotations for the first time...\\n\")\n",
    "        g_coords, g_info, ss2gene = parse_annotation(gtf_annot)\n",
    "        \n",
    "    \n",
    "        for chrom,strand in g_coords:\n",
    "            to_remove_gcoords = set()\n",
    "            to_remove_ginfo = set()\n",
    "            for gene in g_coords[(chrom,strand)]:\n",
    "                if gene[1] in g_info: # check if gene_name is in introns_info keys\n",
    "                    if len(g_info[gene[1]]['stop_codon']) == 0: # if there are no stop codons\n",
    "                        to_remove_gcoords.add(gene) # mark gene for remove\n",
    "                        to_remove_ginfo.add(gene[1]) # mark gene_name for removal\n",
    "\n",
    "            for g in to_remove_gcoords:\n",
    "                g_coords[(chrom,strand)].remove(g)\n",
    "            for g in to_remove_ginfo:\n",
    "                g_info.pop(g)\n",
    "        sys.stdout.write(\"Saving parsed annotations...\\n\")\n",
    "        with open(parsed_gtf, 'wb') as f:\n",
    "            pickle.dump((g_coords, g_info), f)\n",
    "\n",
    "    transcripts_by_gene = tx_by_gene(gtf_annot)\n",
    "\n",
    "    gene_juncs = {}\n",
    "    for chrom,strand in dic_junc:\n",
    "        if (chrom,strand) not in g_coords: \n",
    "            sys.stderr.write(f\"Could not find {chrom} ({strand}) in annotations...\\n\")\n",
    "            continue\n",
    "        juncs = [(x,x) for x in dic_junc[(chrom,strand)]]\n",
    "        juncs.sort()\n",
    "\n",
    "        coords = g_coords[(chrom,strand)]\n",
    "        coords.sort()\n",
    "        \n",
    "        # save junctions that overlapping a gene in gene_juncs dictionary: (gene_name, chrom, strand) : [junctions]\n",
    "        for junc, geneinfo in get_overlap_stream(juncs,coords): \n",
    "            info = (geneinfo[1], chrom,strand)\n",
    "            if info not in gene_juncs:\n",
    "                gene_juncs[info] = []\n",
    "            gene_juncs[info].append(junc[0])\n",
    "\n",
    "    fout = open(f\"{rundir}/{outprefix}_junction_classifications.txt\",'w')\n",
    "    fout.write(\"\\t\".join([\"Gene_name\",\"Intron_coord\",\"Annot\",\"Coding\", \"UTR\", \"Long_exon\", \"Exons_before\", 'Exons_after'])+'\\n')\n",
    "    \n",
    "    for gene_name, chrom, strand in gene_juncs:\n",
    "        sys.stdout.write(f\"Processing {gene_name} ({chrom}:{strand})\\n\")\n",
    "        \n",
    "        query_juncs = gene_juncs[(gene_name,chrom,strand)] # from LeafCutter perind file\n",
    "        if gene_name not in g_info: continue\n",
    "        junctions = g_info[gene_name]['junctions'] # from annotation\n",
    "        \n",
    "        # classify all junctions in gene\n",
    "        junctions = list(junctions.union(query_juncs))\n",
    "\n",
    "        start_codons = g_info[gene_name]['start_codon'] \n",
    "        stop_codons = g_info[gene_name]['stop_codon']\n",
    "\n",
    "        if verbose:\n",
    "            sys.stdout.write(f\"LeafCutter junctions ({len(query_juncs)}) All junctions ({len(junctions)}) Start codons ({len(start_codons)}) Stop codons ({len(stop_codons)}) \\n\")\n",
    "\n",
    "        junc_pass, junc_fail, proteins = solve_NMD(chrom,strand,junctions, \n",
    "                                                   start_codons, stop_codons, \n",
    "                                                   gene_name)\n",
    "        \n",
    "\n",
    "        junc_fail = set(junc_fail.keys())\n",
    "        junc_pass = set(junc_pass.keys())\n",
    "        failing_juncs = junc_fail.difference(junc_pass)\n",
    "\n",
    "        old_junc_pass = junc_pass\n",
    "        junc_pass = {}\n",
    "        junc_pass['normal'] = old_junc_pass\n",
    "        junc_pass['long_exon'] = long_exon_finder(failing_juncs, gene_name, transcripts_by_gene, strand, chrom)\n",
    "        junc_pass['exons_before'], junc_pass['exons_after'] = many_junctions(failing_juncs, gene_name, transcripts_by_gene, strand, chrom)\n",
    "\n",
    "        for j in junctions:\n",
    "            bool_pass = j in junc_pass['normal'] or j in g_info[gene_name]['pcjunctions']\n",
    "            bool_fail = j in junc_fail or j in junc_pass['long_exon']\n",
    "            utr = False\n",
    "            long_exon = j in junc_pass['long_exon']\n",
    "            exons_before = j in junc_pass['exons_before'] \n",
    "            exons_after = j in junc_pass['exons_after'] \n",
    "            if not bool_pass:\n",
    "                # Check that it's not in UTR                \n",
    "                utr = check_utrs(j,g_info[gene_name]['utrs'])\n",
    "\n",
    "            if bool_fail or bool_pass:\n",
    "                tested = True\n",
    "            else:\n",
    "                tested = False\n",
    "            annotated = j in g_info[gene_name]['junctions']\n",
    "            #if not bool_pass and annotated:\n",
    "            #print(\"%s %s %s junction: %s tested: %s utr: %s coding: %s annotated: %s \"%(chrom, strand, gene_name, j, tested,utr, bool_pass, annotated))\n",
    "            \n",
    "            fout.write('\\t'.join([gene_name, f'{chrom}:{j[0]}-{j[1]}',\n",
    "                                  str(annotated), str(bool_pass), str(utr), str(long_exon), str(exons_before), str(exons_after)])+'\\n')\n",
    "        \n",
    "\n",
    "def boolean_to_bit(bool_vec):\n",
    "    # Convert boolean vector to string of \"1\"s and \"0\"s\n",
    "    bin_str = ''.join(['1' if b else '0' for b in bool_vec])\n",
    "    \n",
    "    # Convert this binary string into an integer\n",
    "    # bit_num = int(bin_str, 2)\n",
    "    \n",
    "    return bin_str\n",
    "            \n",
    "def merge_discordant_logics(sjc_file: str):\n",
    "    '''some junctions have multiple classifications. Use conservative approach\n",
    "    to merge them.\n",
    "    '''\n",
    "    sjc = pd.read_csv(sjc_file, sep = \"\\t\")\n",
    "\n",
    "    classifier = {\n",
    "        # each bit represents [ is annotated, is coding, is UTR ]\n",
    "        '000': 'UP', # UnProductive,\n",
    "        '001': 'NE', # NEither, not productive, but not considered unprod. due to close to UTR\n",
    "        '010': 'PR', # PRoductive\n",
    "        '100': 'UP', # UnProductive\n",
    "        '101': 'PR', # PRoductive\n",
    "        '110': 'PR' # PRoductive\n",
    "        }\n",
    "    \n",
    "    # group dt\n",
    "    sjc = sjc.groupby('Intron_coord').agg('max').reset_index()\n",
    "    # convert Annotation, Coding, UTR status to binary strings then to SJ categories\n",
    "    sjc['SJClass'] = sjc.apply(lambda x: boolean_to_bit(x[2:5]), axis=1).map(classifier)\n",
    "    \n",
    "    # convert df to dict\n",
    "    sjc = sjc.set_index('Intron_coord').to_dict(orient='index')\n",
    "    sjc = {tuple([k.split(':')[0]]) + tuple(k.split(':')[1].split('-')): v for k, v in sjc.items()}\n",
    "    sjc = {(k[0], int(k[1]), int(k[2])): v for k, v in sjc.items()}\n",
    "\n",
    "\n",
    "    return sjc\n",
    "    # sjc is a dcitionary with:\n",
    "    # - keys: intron coordinates, e.g. ('chr1', 1000, 2000)\n",
    "    # - values: a dictionary e.g. {'Gene_name': 'DNMBP', 'Annot': False, 'Coding': False, 'UTR': False, 'SJClass': 'UP'})\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def main(options):\n",
    "\n",
    "    if options.countfile is None:\n",
    "        sys.stderr.write(\"Error: no LeafCutter junction file provided...\\npython SpliceJunctionClassifier.py -j Leafcutter_perind.counts.gz\\n\")\n",
    "        exit(0)\n",
    "    \n",
    "    global fa\n",
    "    sys.stdout.write(f\"Loading genome {options.genome} ...\")\n",
    "    fa = pyfastx.Fasta(options.genome)\n",
    "    sys.stdout.write(\"done!\\n\")\n",
    "\n",
    "    ClassifySpliceJunction(options)\n",
    "\n",
    "    # for testing\n",
    "    # sjc_file = f\"{options.rundir}/{options.outprefix}_junction_classifications.txt\"\n",
    "    # print(f\"Merging discordant logics in {sjc_file}...\")\n",
    "    # # print the first 10 items in the merged dictionary\n",
    "    # sjc = merge_discordant_logics(sjc_file)\n",
    "    # print(list(sjc.items())[:10])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import sys\n",
    "    import gzip\n",
    "    import argparse\n",
    "    import pickle\n",
    "    from statistics import mean, median\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from Bio.Seq import Seq\n",
    "    import pyfastx\n",
    "    from bisect import insort\n",
    "    \n",
    "\n",
    "    parser = argparse.ArgumentParser(description='SpliceJunctionClassifier')\n",
    "\n",
    "    parser.add_argument(\"-c\", \"--countfile\", dest=\"countfile\",\n",
    "                  help=\"LeafCutter perind counts file, e.g. leafcutter_perind.counts.gz\")\n",
    "\n",
    "    parser.add_argument(\"-o\", \"--outprefix\", dest=\"outprefix\", default = 'Leaf2',\n",
    "                      help=\"output prefix (default: Leaf2)\")\n",
    "\n",
    "    parser.add_argument(\"-r\", \"--rundir\", dest=\"rundir\", default = '.',\n",
    "                      help=\"run directory (default: .)\")\n",
    "\n",
    "    parser.add_argument(\"-A\", \"--annotation\", dest=\"annot\",\n",
    "                  help=\"Annotation GTF file, for example gencode.v37.annotation.gtf.gz\")\n",
    "    \n",
    "    parser.add_argument(\"-G\", \"--genome\", dest=\"genome\",\n",
    "                  help=\"Reference genome fasta file.\")\n",
    "\n",
    "    parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\", default = False,\n",
    "                      help=\"verbose mode\")\n",
    "                \n",
    "    options = parser.parse_args()\n",
    "    \n",
    "    main(options)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
